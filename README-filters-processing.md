# Обработка фильтров из продуктов

Этот набор скриптов предназначен для обработки данных из коллекции `products` и создания фильтров на основе поля `productValues`.

## Описание

Скрипты читают данные из коллекции `products`, извлекают поля `hlSectionId` и `productValues`, обрабатывают массив `edges` в `productValues` и извлекают `node.title` и `node.value`. На основе полученных данных создаются уникальные фильтры для каждого `hlSectionId`.

## Структура данных

### Входные данные (коллекция products)
```javascript
{
  hlSectionId: Number,
  productValues: {
    edges: [
      {
        node: {
          title: String,  // Название фильтра
          value: String   // Значение фильтра
        }
      }
    ]
  }
}
```

### Выходные данные (коллекция filters)
```javascript
{
  sectionId: Number,
  node: {
    filter: String,    // Уникальное название фильтра
    values: [String]   // Массив уникальных значений
  }
}
```

## Скрипты

### 1. process-filters.js
Прямая обработка продуктов и сохранение фильтров в базу данных.

**Запуск:**
```bash
npm run process-filters
# или
node process-filters.js
```

**Функции:**
- Читает данные из коллекции `products`
- Группирует продукты по `hlSectionId`
- Извлекает уникальные фильтры из `productValues`
- Сохраняет фильтры в коллекцию `filters`

### 2. export-filters-json.js
Экспорт фильтров в JSON файл с последующей загрузкой в базу данных.

**Запуск:**
```bash
npm run export-filters
# или
node export-filters-json.js
```

**Функции:**
- Обрабатывает продукты и создает JSON файл
- Сохраняет данные в файл `filters.json`
- Загружает данные в коллекцию `filters`

### 3. process-filters-stream.js ⭐ РЕКОМЕНДУЕТСЯ
Потоковая обработка продуктов для больших объемов данных.

**Запуск:**
```bash
npm run process-filters-stream
# или
node process-filters-stream.js
```

**Функции:**
- Обрабатывает данные пакетами для экономии памяти
- Обрабатывает секции по отдельности
- Подходит для больших баз данных
- Избегает ошибок превышения лимита памяти

### 4. export-filters-stream.js ⭐ РЕКОМЕНДУЕТСЯ
Потоковый экспорт фильтров в JSON файл.

**Запуск:**
```bash
npm run export-filters-stream
# или
node export-filters-stream.js
```

**Функции:**
- Потоковая обработка для больших объемов данных
- Создает JSON файл без загрузки всех данных в память
- Загружает данные в коллекцию `filters`
- Подходит для больших баз данных

## Формат JSON файла

Создаваемый JSON файл имеет следующую структуру:

```json
[
  {
    "sectionId": 168,
    "node": {
      "filter": "vendor",
      "values": ["OnePlus", "Apple", "Samsung"]
    }
  },
  {
    "sectionId": 168,
    "node": {
      "filter": "series",
      "values": ["Ace 5", "iPhone 15", "Galaxy S24"]
    }
  }
]
```

## Логика обработки

1. **Группировка по секциям**: Продукты группируются по полю `hlSectionId`
2. **Извлечение фильтров**: Для каждого продукта извлекаются фильтры из `productValues.edges`
3. **Уникализация**: Собираются уникальные названия фильтров (`title`) и уникальные значения (`value`)
4. **Создание объектов**: Формируются объекты в требуемом формате
5. **Сохранение**: Данные сохраняются в коллекцию `filters`

## Статистика

Скрипты выводят подробную статистику обработки:

- Общее количество продуктов
- Количество обработанных продуктов
- Количество уникальных секций
- Количество созданных фильтров
- Количество ошибок

## Требования

- Node.js
- MongoDB
- Зависимости из `package.json`
- Настроенный файл `.env` с `MONGO_URI`

## Пример использования

```bash
# Обработка фильтров напрямую
npm run process-filters

# Экспорт в JSON и загрузка в базу
npm run export-filters
```

## Обработка ошибок

Скрипты включают обработку ошибок:
- Проверка существования полей
- Валидация структуры данных
- Логирование ошибок
- Продолжение работы при частичных ошибках

## Производительность

- Использует MongoDB агрегации для эффективной группировки
- Обрабатывает данные пакетами
- Минимизирует количество запросов к базе данных
- Поддерживает большие объемы данных

## Решение проблем с памятью

### Проблема
При обработке больших объемов данных может возникнуть ошибка:
```
PlanExecutor error during aggregation :: caused by :: Used too much memory for a single array
```

### Решение
Используйте потоковые версии скриптов:
- `process-filters-stream.js` - для прямой обработки
- `export-filters-stream.js` - для экспорта в JSON

**Особенности потоковых скриптов:**
- Обрабатывают секции по отдельности
- Используют пакетную обработку (по 100 продуктов за раз)
- Не загружают все данные в память одновременно
- Подходят для баз данных любого размера 